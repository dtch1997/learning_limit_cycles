{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized CPGs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "First, we import the required libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "\n",
    "from vector_field import vector_field, utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To model the tangential flow, we construct a simple counterclockwise rotational field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a general class of functions which define the\n",
    "# tangential component of the CPG update. \n",
    "\n",
    "class SimpleRotationalField(vector_field.VectorField):\n",
    "    def __init__(self):\n",
    "        pass \n",
    "    def get_gradient(self,x):\n",
    "        theta = np.arctan2(x[0], x[1])\n",
    "        return np.array([-np.cos(theta), np.sin(theta)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing a basic CPG\n",
    "\n",
    "Now we're ready to combine the above elements to construct a CPG out of base components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square = lambda x: jnp.dot(x, x)\n",
    "inv_sq = lambda x: 1 / jnp.dot(x, x)\n",
    "s1 = vector_field.FunctionalPotentialField(square)\n",
    "s2 = vector_field.FunctionalPotentialField(inv_sq)\n",
    "s3 = vector_field.LinearCombinationPotentialField([s1, s2])\n",
    "\n",
    "m = SimpleRotationalField()\n",
    "d = vector_field.LinearCombinationVectorField([s3, m])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We simulate the CPG update for 100 steps with step size of 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = utilities.simulate_trajectory(\n",
    "    d, jnp.array([0.5, 0.5]),\n",
    "    step_size = 0.1, num_iters = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we visualize the resulting trajectory. \n",
    "As we can see, we have constructed a system with stable limit cycle at ```x^2 + y^2 = 1```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(x_history, **subplot_kwargs):\n",
    "    fig, ax = plt.subplots(**subplot_kwargs)\n",
    "    ax.scatter(x_history[:,0], x_history[:,1])\n",
    "    ax.grid(True)\n",
    "\n",
    "plot_history(history, figsize=(8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Transformations\n",
    "\n",
    "We consider linear transformations of 2D space. The transform shown below warps the circle into an ellipse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = jnp.array([[1.0, -0.5],[-0.5, 1.0]])\n",
    "\n",
    "def scatter_circle_points():\n",
    "    n = 100\n",
    "    x = np.zeros((100,2))\n",
    "    phases = np.linspace(0, 2*np.pi, n)\n",
    "    for i in range(n):\n",
    "        x[i] = np.array([np.cos(phases[i]), np.sin(phases[i])])\n",
    "    return x \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "circle_points = scatter_circle_points()\n",
    "ax.scatter(circle_points[:,0], circle_points[:,1])\n",
    "ellipse_points = circle_points @ A.T\n",
    "ax.scatter(ellipse_points[:,0], ellipse_points[:,1])\n",
    "rot_ellipse_points = ellipse_points @ utilities.get_rotational_matrix(np.pi/2).T\n",
    "ax.scatter(rot_ellipse_points[:,0], rot_ellipse_points[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: jnp.linalg.inv(A) @ x\n",
    "ellipse = vector_field.SmoothTransformationVectorField(d, f)\n",
    "\n",
    "history = utilities.simulate_trajectory(\n",
    "    ellipse, jnp.array([0.5, 0.5]),\n",
    "    step_size = 0.1, num_iters = 100, grad_clip=0.1)\n",
    "plot_history(history, figsize=(8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can construct a rotated form of the above elliptical shape with linear transformation as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = lambda x: np.linalg.inv(utilities.get_rotational_matrix(np.pi/2) @ A) @ x\n",
    "rot_ellipse = vector_field.SmoothTransformationVectorField(d, g)\n",
    "\n",
    "history = utilities.simulate_trajectory(\n",
    "    rot_ellipse, jnp.array([0.5, 0.5]),\n",
    "    step_size = 0.1, num_iters = 100, grad_clip=0.1)\n",
    "plot_history(history, figsize=(8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the potential fields used to obtain these elliptical limit cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_f = vector_field.SmoothTransformationPotentialField(s3, f)\n",
    "utilities.plot_potential_field(s3_f, jnp.array([-1, 1]), jnp.array([-1, 1]), max_clip = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_g = vector_field.SmoothTransformationPotentialField(s3, g)\n",
    "utilities.plot_potential_field(s3_g, jnp.array([-1, 1]), jnp.array([-1, 1]), max_clip = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing A Complex Limit Cycle\n",
    "\n",
    "With the above building blocks, we can now try to build a dynamical system that exhibits a more complex limit cycle.\n",
    "\n",
    "By superimposing two elliptical potential fields in a cross-shape and the rotational field previously given, we theorize that we can construct a clover shape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clover = vector_field.LinearCombinationVectorField(\n",
    "    [ellipse, rot_ellipse]\n",
    ")\n",
    "\n",
    "history = utilities.simulate_trajectory(\n",
    "    clover, jnp.array([0.5, 0.5]),\n",
    "    step_size = 0.1, num_iters = 100, grad_clip=0.1)\n",
    "plot_history(history, figsize=(8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That worked surprisingly well. Overall the shape seems to be 'rotated' slightly from the ideal X-shape. Not entirely sure why that happens but I think it is due to the fact that we generated ```rot_ellipse``` as a 90-degree rotation of ```ellipse``` - some of that rotation is preserved in the linear combination.\n",
    "\n",
    "Nonetheless, we observe four distinct corners. \n",
    "\n",
    "Let's try a wider variety of starting conditions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_conditions = [\n",
    "    np.random.uniform(-1.2, 1.2, size = (2)) for i in range(5)\n",
    "]\n",
    "x_history = []\n",
    "for s in starting_conditions:\n",
    "    history = utilities.simulate_trajectory(\n",
    "        clover, s,\n",
    "        step_size = 0.02, num_iters = 400, grad_clip=0.1)\n",
    "    x_history.append(history)\n",
    "\n",
    "def plot_histories(x_history, **subplot_kwargs):\n",
    "    \"\"\"\n",
    "    x_history: A list of histories\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(**subplot_kwargs)\n",
    "    for x_hist in x_history:\n",
    "        ax.plot(x_hist[:,0], x_hist[:,1])\n",
    "        ax.grid(True)\n",
    "\n",
    "plot_histories(x_history, figsize=(8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a bit rough but the seeds of something very cool are here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Limit Cycles\n",
    "\n",
    "So far we have designed functions that warp the state space in some analytic way. \n",
    "\n",
    "Now we want to see if we can learn simple transformations that deform the circular limit cycle into an arbitrary shape, e.g a five-pointed star shape. \n",
    "\n",
    "This involves a supervised learning task. Minimally, we need to construct pairs of points on the base and target limit cycles, and learn a smooth (preferably invertible) diffeomorphism between them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax\n",
    "import optax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As a warm-up, let's use points on the limit cycle obtained from our previous construction and pair them with points on the circle (of similar phase). \n",
    "\n",
    "To do this, we initialize a trajectory, allow it to converge, and then collect 1000 data points associated with phases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset():\n",
    "    history = utilities.simulate_trajectory(\n",
    "        clover, np.array([1.0, 1.0]),\n",
    "        step_size = 0.02, num_iters = 1200, grad_clip=0.1\n",
    "    )\n",
    "    # Cut off the off-limit-cycle points\n",
    "    return history[200:]\n",
    "\n",
    "targets = get_dataset()\n",
    "plot_history(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phases = np.arctan2(targets[:,0], targets[:,1])\n",
    "features = np.zeros([1000, 2])\n",
    "features[:,0] = np.cos(phases)\n",
    "features[:,1] = np.sin(phases)\n",
    "\n",
    "plot_history(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing ML in Flax\n",
    "\n",
    "Adapted from https://flax.readthedocs.io/en/latest/notebooks/annotated_mnist.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import flax.linen as nn\n",
    "from flax.training import train_state \n",
    "\n",
    "# 2. Define Network\n",
    "class Net(nn.Module):\n",
    "    \"\"\" Construct a learnable, nonlinear diffeomorphism on R^2\"\"\"\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(features = 8)(x)\n",
    "        x = nn.elu(x)\n",
    "        x = nn.Dense(features = 2)(x)\n",
    "        return x\n",
    "\n",
    "# 3. Define loss\n",
    "def mse_loss(*, preds, targets):\n",
    "    return optax.l2_loss(preds, targets).mean()\n",
    "\n",
    "# 4. Define metrics - TODO\n",
    "def compute_metrics(*, preds, targets):\n",
    "  loss = mse_loss(preds=preds, targets=targets)\n",
    "  metrics = {\n",
    "      'loss': loss,\n",
    "  }\n",
    "  return metrics\n",
    "\n",
    "# 5. Define dataset\n",
    "def get_datasets(features, targets):\n",
    "    # Split train and val sets\n",
    "    X_train = features[:800]\n",
    "    X_test = features[800:]\n",
    "    y_train = targets[:800]\n",
    "    y_test = targets[800:]\n",
    "    \n",
    "    train_ds = {\n",
    "        'features': X_train, \n",
    "        'targets': y_train\n",
    "    }\n",
    "    test_ds = {\n",
    "        'features': X_test, \n",
    "        'targets': y_test,\n",
    "    }\n",
    "    return train_ds, test_ds\n",
    "\n",
    "\n",
    "# 6. Train state\n",
    "def create_train_state(rng, learning_rate, momentum):\n",
    "    net = Net()\n",
    "    params = net.init(rng, jnp.ones([1, 2]))['params']\n",
    "    tx = optax.sgd(learning_rate, momentum)\n",
    "    return train_state.TrainState.create(\n",
    "        apply_fn=net.apply, params=params, tx=tx)\n",
    "\n",
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "    \"\"\"Train for a single step.\"\"\"\n",
    "    def loss_fn(params):\n",
    "        preds = Net().apply({'params': params}, batch['features'])\n",
    "        loss = mse_loss(preds=preds, targets=batch['targets'])\n",
    "        return loss, preds\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (_, preds), grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    metrics = compute_metrics(preds=preds, targets=batch['targets'])\n",
    "    return state, metrics\n",
    "\n",
    "@jax.jit\n",
    "def eval_step(params, batch):\n",
    "    preds = Net().apply({'params': params}, batch['features'])\n",
    "    return compute_metrics(preds=preds, targets=batch['targets'])\n",
    "\n",
    "def train_epoch(state, train_ds, batch_size, epoch, rng):\n",
    "    \"\"\"Train for a single epoch.\"\"\"\n",
    "    train_ds_size = len(train_ds['features'])\n",
    "    steps_per_epoch = train_ds_size // batch_size\n",
    "\n",
    "    perms = jax.random.permutation(rng, train_ds_size)\n",
    "    perms = perms[:steps_per_epoch * batch_size]  # skip incomplete batch\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "    batch_metrics = []\n",
    "    for perm in perms:\n",
    "        batch = {k: v[perm, ...] for k, v in train_ds.items()}\n",
    "        state, metrics = train_step(state, batch)\n",
    "        batch_metrics.append(metrics)\n",
    "\n",
    "    # compute mean of metrics across each batch in epoch.\n",
    "    batch_metrics_np = jax.device_get(batch_metrics)\n",
    "    epoch_metrics_np = {\n",
    "        k: np.mean([metrics[k] for metrics in batch_metrics_np])\n",
    "        for k in batch_metrics_np[0]}\n",
    "\n",
    "    print('train epoch: %d, loss: %.4f ' % (\n",
    "        epoch, epoch_metrics_np['loss']))\n",
    "\n",
    "    return state\n",
    "\n",
    "def eval_model(params, test_ds):\n",
    "    metrics = eval_step(params, test_ds)\n",
    "    metrics = jax.device_get(metrics)\n",
    "    summary = jax.tree_map(lambda x: x.item(), metrics)\n",
    "    return summary['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, test_ds = get_datasets(features, targets)\n",
    "rng = jax.random.PRNGKey(0)\n",
    "rng, init_rng = jax.random.split(rng)\n",
    "learning_rate = 0.1\n",
    "momentum = 0.9\n",
    "state = create_train_state(init_rng, learning_rate, momentum)\n",
    "del init_rng  # Must not be used anymore.\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "  # Use a separate PRNG key to permute image data during shuffling\n",
    "  rng, input_rng = jax.random.split(rng)\n",
    "  # Run an optimization step over a training batch\n",
    "  state = train_epoch(state, train_ds, batch_size, epoch, input_rng)\n",
    "  # Evaluate on the test set after each training epoch \n",
    "  test_loss = eval_model(state.params, test_ds)\n",
    "  print(' test epoch: %d, loss: %.2f ' % (\n",
    "      epoch, test_loss))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "58794497aaf46abe5d1fab5cc23916a38f79a233f4b3f254f0d7e43193129b64"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('general_cpg': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
